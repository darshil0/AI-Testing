[build-system]
requires = ["setuptools>=61.0"]
build-backend = "setuptools.build_meta"

[project]
name = "ai-evaluation-framework"
version = "2.1.0"
description = "A professional, enterprise-ready evaluation framework for benchmarking AI models."
authors = [{ name = "Darshil", email = "darshil@example.com" }]
license = { file = "LICENSE" }
readme = "README.md"
classifiers = [
    "Programming Language :: Python :: 3",
    "License :: OSI Approved :: MIT License",
    "Operating System :: OS Independent",
]
dependencies = [
    "openai>=1.2.0",
    "anthropic>=0.21.0",
    "google-generativeai>=0.4.0",
    "requests>=2.31.0",
    "python-dotenv>=1.0.0",
    "pydantic>=2.0.0",
    "ollama>=0.1.0",
    "datasets>=2.16.0",
    "rich>=13.0.0",
    "streamlit>=1.30.0",
    "matplotlib>=3.8.0",
    "seaborn>=0.13.0",
    "pandas>=2.0.0",
    "numpy>=1.24.0",
    "tenacity>=8.2.0",
    "pytest>=7.0.0",
    "pytest-mock>=3.6.0",
    "pyyaml>=6.0.0",
    "flake8>=6.0.0",
    "python-dateutil>=2.8.2",
    "jsonschema>=4.17.0",
]

[project.scripts]
run-evaluation = "ai_evaluation.run_evaluation:main"
view-dashboard = "ai_evaluation.dashboard:main"

[project.urls]
Homepage = "https://github.com/darshil0/AI-Testing"
Issues = "https://github.com/darshil0/AI-Testing/issues"

[tool.setuptools.packages.find]
where = ["."]
include = ["ai_evaluation*"]
